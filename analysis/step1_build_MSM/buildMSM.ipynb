{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step1_build_MSM\n",
    "This Jupyter notebook coverts featurized trajectories in `insulin-dimer/step0_featurize/outputs` into discrete state trajectories (`state_arr`)\n",
    "\n",
    "Following are the tasks this notebook does:\n",
    "- Concatenating featurized trajectories `cvs_df`:\n",
    "I first ran 2.5 ns of 18816 (28x28x24) trajectories, and I extended them to 5.0 ns. \n",
    "This leads to some CVs being computed separately for first and last 2.5 ns, while others were computed as a whole (5ns)\n",
    "- Definition of dimer and separte monomer state: `dim_arr`, ($A$), `mon_arr` ($B$)\n",
    "- Select and scale CVs to convert them to features for defining Markov states based on $k$-mean clustering `proc_df`\n",
    "- Define Markov states `state_arr` by running $k$-means on `proc_df` space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Open only if necessary. Code can be highly confusing.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "\n",
    "# 1. cvs_cat_df\n",
    "## cvs_0_df\n",
    "workdir=f\"/project/dinner/kjeong/insulin/pipeline\"\n",
    "cvs_0_arr = np.load(f\"{workdir}/step2_procfeat/CV_data.npy\")\n",
    "cvs_0_label = np.load(f\"{workdir}/step2_procfeat/CV_label.npy\")\n",
    "cvs_0_df=pd.DataFrame({key: value for key, value in zip(cvs_0_label, cvs_0_arr.T)})\n",
    "\n",
    "Nframe = cvs_0_arr.shape[0]\n",
    "ntraj = 28*28*24\n",
    "length = 1000\n",
    "\n",
    "## cvs_1_df\n",
    "input_path=f\"{workdir}/step1_feat/output/extend_5ns/\"\n",
    "\n",
    "with open(f\"{input_path}/HeavyContact.pkl\", 'rb') as f:\n",
    "    HeavyContact_ls = dill.load(f)\n",
    "HeavyContact = np.hstack([tmp.timeseries for tmp in HeavyContact_ls])\n",
    "\n",
    "with open(f\"{input_path}/IRMSD.pkl\", 'rb') as f:\n",
    "    IRMSD_ls = dill.load(f)\n",
    "IRMSD = np.hstack([tmp.rmsd[:, 2] for tmp in IRMSD_ls])\n",
    "\n",
    "with open(f\"{input_path}/ISolv.pkl\", 'rb') as f:\n",
    "    ISolv_ls = dill.load(f)\n",
    "NSharedWater = np.hstack([tmp.timeseries[:, 0, 1] for tmp in ISolv_ls])\n",
    "NWater = np.hstack([tmp.timeseries[:, 0, 0] for tmp in ISolv_ls])\n",
    "\n",
    "with open(f\"{input_path}/angle_open.pkl\", 'rb') as f:\n",
    "    angle_open_ls = dill.load(f)\n",
    "Angle_Open = np.vstack([(180/np.pi) * tmp[1].timeseries for tmp in angle_open_ls]) # (9408000, 2)\n",
    "\n",
    "with open(f\"{input_path}/angle.pkl\", 'rb') as f:\n",
    "    angle_ls = dill.load(f)\n",
    "Angle = np.vstack([(180/np.pi) * tmp[0].timeseries for tmp in angle_ls]) # (9408000, 4) \"phi-alpha\", \"phi-alpha(me)\", \"phi-beta\", \"phi-beta(me)\",\n",
    "\n",
    "with open(f\"{input_path}/Euler.pkl\", 'rb') as f:\n",
    "    Euler_ls = dill.load(f)\n",
    "Euler = np.vstack([tmp.timeseries for tmp in Euler_ls]) # (9408000, 6) \"r_com, \"theta\", \"phi\", \"Theta\", \"Phi\", \"Psi\"\"\n",
    "Euler[:, 0] = Euler[:, 0] / 10\n",
    "Euler[:, 1:] = Euler[:, 1:] * 180/np.pi\n",
    "\n",
    "with open(f\"{input_path}/BBagchi.pkl\", 'rb') as f:\n",
    "    BBagchi_ls = dill.load(f)\n",
    "R_COM_Bagchi = []\n",
    "N_Bagchi = []\n",
    "for r_com_tmp, n_tmp in BBagchi_ls:\n",
    "    R_COM_Bagchi.append(r_com_tmp.timeseries)\n",
    "    N_Bagchi.append(n_tmp.timeseries)\n",
    "R_COM_Bagchi = np.hstack(R_COM_Bagchi)/10\n",
    "N_Bagchi = np.hstack(N_Bagchi)\n",
    "#'R_com', 'CrossContact(7A)'\n",
    "\n",
    "cvs_1_df = pd.DataFrame(\n",
    "    {\n",
    "        'phi-alpha': Angle[:, 0],\n",
    "        'phi-alpha(me)': Angle[:, 1],\n",
    "        'phi-beta': Angle[:, 2],\n",
    "        'phi-beta(me)': Angle[:, 3],\n",
    "        \"phi-open(ch1)\": Angle_Open[:, 0],\n",
    "        \"phi-open(ch2)\": Angle_Open[:, 1],\n",
    "        'r_com': Euler[:, 0],\n",
    "        'theta': Euler[:, 1],\n",
    "        'phi': Euler[:, 2],\n",
    "        'Theta': Euler[:, 3],\n",
    "        'Phi': Euler[:, 4],\n",
    "        'Psi': Euler[:, 5],\n",
    "        'Nwater(Interface)': NWater,\n",
    "        'Shared Nwater(Interface)': NSharedWater,\n",
    "        'R_com': R_COM_Bagchi,\n",
    "        'CrossContact(7A)': N_Bagchi\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['phi-alpha', 'phi-alpha(me)', 'phi-beta', 'phi-beta(me)',\n",
      "       'phi-open(ch1)', 'phi-open(ch2)', 'r_com', 'theta', 'phi', 'Theta',\n",
      "       'Phi', 'Psi', 'Nwater(Interface)', 'Shared Nwater(Interface)', 'R_com',\n",
      "       'CrossContact(7A)', 'zip-Dist', 'phi-open'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 2. raw_feat_cat_df\n",
    "## a. Update cvs_0,1_df for `beta`, `alpha`, `zip-Dist``\n",
    "## b. Use to make #4. `feat_cat_arr` to define `state_arr`\n",
    "with open(f\"{workdir}/step1_feat/output/distance.pkl\", 'rb') as f:\n",
    "    ref_feat = dill.load(f)\n",
    "feat_0_arr=np.concatenate([dt.timeseries for dt in ref_feat])/10 #A to nm\n",
    "\n",
    "with open(f\"{input_path}/distance.pkl\", 'rb') as f:\n",
    "    raw_feat = dill.load(f)\n",
    "feat_1_arr=np.concatenate([dt.timeseries for dt in raw_feat])/10 #A to nm\n",
    "\n",
    "beta = np.sum(feat_1_arr[:, [6, 7, 8], [8, 7, 6]], axis=1)/3\n",
    "alpha = np.sum(feat_1_arr[:, [0, 0, 1, 2, 2, 3, 3], [2, 3, 3, 2, 0, 0, 1]], axis=1)/7\n",
    "\n",
    "###a2. zip, open\n",
    "cvs_0_df['zip-Dist'] = np.sum(feat_0_arr[:, [-1, -2], [4, 3]] - feat_0_arr[:, [4, 3], [-1, -2]], axis=1)\n",
    "cvs_1_df['zip-Dist'] = np.sum(feat_1_arr[:, [-1, -2], [4, 3]] - feat_1_arr[:, [4, 3], [-1, -2]], axis=1)\n",
    "\n",
    "cvs_0_df['phi-open'] = cvs_0_df['phi-open(ch1)'] + cvs_0_df['phi-open(ch2)']\n",
    "cvs_1_df['phi-open'] = cvs_1_df['phi-open(ch1)'] + cvs_1_df['phi-open(ch2)']\n",
    "\n",
    "### cvs_df Concateante\n",
    "cvs_dic = {}\n",
    "for key in cvs_1_df.keys():\n",
    "    cvs_dic[key] = np.ravel(np.hstack((\n",
    "        cvs_0_df.loc[:, key].to_numpy().reshape(ntraj, int(length/2)),\n",
    "        cvs_1_df.loc[:, key].to_numpy().reshape(ntraj, int(length/2))\n",
    "    )))\n",
    "cvs_df = pd.DataFrame(cvs_dic)\n",
    "print(cvs_df.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. dim_arr, mon_arr, other_arr\n",
    "# dim_0_arr, mon_0_arr, other_0_arr\n",
    "tpt_arr = np.load(f\"{workdir}/step3_DGA/tpt_data.npy\")\n",
    "tpt_label= np.load(f\"{workdir}/step3_DGA/tpt_label.npy\")\n",
    "tpt_0_df=pd.DataFrame({key: value for key, value in zip(tpt_label, tpt_arr.T)})\n",
    "\n",
    "dim_0_arr = tpt_0_df.state.to_numpy().astype(int)==0\n",
    "mon_0_arr = tpt_0_df.state.to_numpy().astype(int)==199\n",
    "other_0_arr = ~(dim_0_arr | mon_0_arr)\n",
    "\n",
    "# dim_1_arr, mon_1_arr, other_1_arr\n",
    "dim_1_arr = (IRMSD<2) & (Angle[:, 0]>120) & (Angle[:, 0]<135) & (beta<0.55) & (alpha<0.8)\n",
    "mon_1_arr = (IRMSD>10) & (HeavyContact==0) & (NSharedWater == 0)\n",
    "other_1_arr = ~(dim_1_arr | mon_1_arr)\n",
    "\n",
    "# Concatenate\n",
    "dim_arr = np.ravel(np.hstack([dim_0_arr.reshape(28*28*24, 500), dim_1_arr.reshape(28*28*24, 500)]))\n",
    "mon_arr = np.ravel(np.hstack([mon_0_arr.reshape(28*28*24, 500), mon_1_arr.reshape(28*28*24, 500)]))\n",
    "other_arr = np.ravel(np.hstack([other_0_arr.reshape(28*28*24, 500), other_1_arr.reshape(28*28*24, 500)]))\n",
    "#np.save(\"/project/dinner/kjeong/insulin/pipeline/step7_5ns/step1_cvs_state/AB_arr/dim_arr.npy\", dim_arr)\n",
    "#np.save(\"/project/dinner/kjeong/insulin/pipeline/step7_5ns/step1_cvs_state/AB_arr/mon_arr.npy\", mon_arr)\n",
    "#np.save(\"/project/dinner/kjeong/insulin/pipeline/step7_5ns/step1_cvs_state/AB_arr/other_arr.npy\", other_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18816, 1000, 10, 10)\n",
      "(18816000, 10, 10)\n"
     ]
    }
   ],
   "source": [
    "feat_arr = np.concatenate((\n",
    "    feat_0_arr.reshape((ntraj, int(length/2), 10, 10)),\n",
    "    feat_1_arr.reshape((ntraj, int(length/2), 10, 10)),), axis=1)\n",
    "print(feat_arr.shape)\n",
    "feat_arr = feat_arr.reshape(int(ntraj*length), 10, 10)\n",
    "print(feat_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_id = ([6, 7, 8], [8, 7, 6]) #3\n",
    "alpha_id = ([0, 0, 1, 2, 2, 3, 3], [2, 3, 3, 2, 0, 0, 1]) #7\n",
    "gamma1_id = ([1, 1, 3, 3, 4, 4], [8, 9, 8, 9, 8, 9]) #6\n",
    "gamma2_id = ([8, 9, 8, 9, 8, 9], [1, 1, 3, 3, 4, 4]) #6\n",
    "\n",
    "cvs_df[\"beta\"] = np.sum(feat_arr[:, beta_id[0], beta_id[1]], axis=1)/3\n",
    "cvs_df[\"alpha\"] = np.sum(feat_arr[:, alpha_id[0], alpha_id[1]], axis=1)/7\n",
    "cvs_df[\"gamma1\"] = np.sum(feat_arr[:, gamma1_id[0], gamma1_id[1]], axis=1)/6\n",
    "cvs_df[\"gamma2\"] = np.sum(feat_arr[:, gamma2_id[0], gamma2_id[1]], axis=1)/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18816000, 10) (18816000, 12)\n"
     ]
    }
   ],
   "source": [
    "contact_ids = np.hstack((beta_id, alpha_id, gamma1_id, gamma2_id))\n",
    "nat_mean = np.mean(feat_arr[dim_arr][:, contact_ids[0, :10], contact_ids[1, :10]], axis=0)\n",
    "nat_dist = feat_arr[:, contact_ids[0, :10], contact_ids[1, :10]] - nat_mean\n",
    "nat_mask = nat_dist < 0\n",
    "nat_contact = np.exp(-nat_dist**2/(2*(0.6**2)))\n",
    "nat_contact[nat_mask] = 1\n",
    "\n",
    "nonnat_mean = np.mean(nat_mean)\n",
    "nonnat_dist = feat_arr[:, contact_ids[0, 10:], contact_ids[1, 10:]] - nonnat_mean\n",
    "nonnat_mask = nonnat_dist < 0\n",
    "nonnat_contact = np.exp(-nonnat_dist**2/(2*(0.6**2)))\n",
    "nonnat_contact[nonnat_mask] = 1\n",
    "\n",
    "print(nat_contact.shape, nonnat_contact.shape)\n",
    "\n",
    "cvs_df['betac'] = np.sum(nat_contact[:, :3], axis=1)/3\n",
    "cvs_df['alphac'] = np.sum(nat_contact[:, 3:], axis=1)/7\n",
    "cvs_df['gamma1c'] = np.sum(nonnat_contact[:, :6], axis=1)/6\n",
    "cvs_df['gamma2c'] = np.sum(nonnat_contact[:, 6:], axis=1)/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Zipping\n",
    "input_path_whole=f\"{workdir}/step1_feat/output/whole_5ns/\"\n",
    "with open(f\"{input_path_whole}/ZIP.pkl\", 'rb') as f:\n",
    "    ZIP_ls = dill.load(f)\n",
    "Zip = (180/np.pi)*np.vstack([zip_ls.timeseries for zip_ls in ZIP_ls]).T #(4, nframe) # 4 angles (PROB, 21), (PROB, 8 29), (PROD, 21), (PROD, 8 29)\n",
    "cvs_df[\"zip-turn\"] = Zip[0]+(np.pi-Zip[2])\n",
    "cvs_df[\"zip-term\"] = Zip[1]+(np.pi-Zip[3])\n",
    "#cvs_df[\"zip-turn(D)\"] = Zip[2]+(np.pi-Zip[0])\n",
    "#cvs_df[\"zip-term(D)\"] = Zip[3]+(np.pi-Zip[])\n",
    "\n",
    "# 2. All Contact\n",
    "with open(f\"{input_path_whole}/Allcontact_native.pkl\", 'rb') as f:\n",
    "    allcontact_dic = dill.load(f)\n",
    "Contact = np.vstack([allcontact for allcontact in allcontact_dic.values()]) #(4, nframe) (All, native, semi-native, non-native)\n",
    "for arr, key in zip(Contact, [\"All\", \"native\", \"semi-native\", \"non-native\"]):\n",
    "    cvs_df[key] = arr\n",
    "\n",
    "with open(f\"{input_path_whole}/Detach.pkl\", 'rb') as f:\n",
    "    detach_ls = dill.load(f)\n",
    "DETACH = np.vstack([tmp.timeseries for tmp in detach_ls]) * 180 / np.pi\n",
    "cvs_df[\"detach1\"] = DETACH[:, 0]\n",
    "cvs_df[\"detach2\"] = DETACH[:, 1]\n",
    "\n",
    "with open(f\"{input_path_whole}/BBagchiQ.pkl\", 'rb') as f:\n",
    "    bbagchiQ_ls = dill.load(f)\n",
    "BBagchiQ = np.hstack([tmp.timeseries for tmp in bbagchiQ_ls])\n",
    "cvs_df[\"Q_Bagchi\"] = BBagchiQ\n",
    "\n",
    "# 3. Extra hassle with hydrogen bond on 24-26', 26-24'\n",
    "\n",
    "type_hb = [(0, '24', '24'), (0, '24', '26'), (0, '26', '24'), (0, '26', '26'),\\\n",
    "          (1, '24', '24'), (1, '24', '26'), (1, '26', '24'), (1, '26', '26'),]\n",
    "ntype_hb=len(type_hb)\n",
    "type_hb_label= []\n",
    "for o_tmp, ch1_tmp, ch2_tmp in type_hb:\n",
    "    BondorBridge = \"Hbridge\" if o_tmp else \"Hbond\"\n",
    "    type_hb_label.append(f\"{BondorBridge}( {ch1_tmp}-{ch2_tmp}' )\")\n",
    "\n",
    "\n",
    "hbb0= np.load(f\"{workdir}/step1_feat/output/hbb_arr.npy\")\n",
    "hbb1= np.load(f\"{workdir}/step1_feat/output/extend_5ns/hbb_arr.npy\")\n",
    "nathb_cnt_arr_ls = []\n",
    "for hbb in [hbb0, hbb1]:\n",
    "    nathb_arr = np.zeros((ntype_hb, hbb.shape[0]), dtype=bool)\n",
    "    for i0, (o_tmp, ch1_tmp, ch2_tmp) in enumerate(type_hb):\n",
    "        bool_o_tmp= (hbb[:, 1] == o_tmp)\n",
    "        bool_ch_tmp = np.zeros((2, len(bool_o_tmp)),dtype=bool)\n",
    "        for i1, (i2, ch_str) in enumerate(zip([2, 3], [ch1_tmp, ch2_tmp])):\n",
    "            if ch_str == '24':\n",
    "                bool_ch_tmp[i1] = (hbb[:, i2] < 2)\n",
    "            elif ch_str == '26':\n",
    "                bool_ch_tmp[i1] = (hbb[:, i2]> 3)\n",
    "            else:\n",
    "                raise KeyError\n",
    "        nathb_arr[i0]=bool_o_tmp & bool_ch_tmp[0] & bool_ch_tmp[1]\n",
    "    nathb_cnt_arr = np.zeros((ntype_hb, Nframe), dtype=np.int8)\n",
    "    for i0 in range(ntype_hb):\n",
    "        uq_tmp, cnt_tmp=np.unique(hbb[np.nonzero(nathb_arr[i0])[0], 0], return_counts=True)\n",
    "        nathb_cnt_arr[i0, uq_tmp] = cnt_tmp\n",
    "    nathb_cnt_arr_ls.append(nathb_cnt_arr)\n",
    "\n",
    "nathb_cnt_arr = np.concatenate(\n",
    "    (nathb_cnt_arr_ls[0].reshape(-1, ntraj, int(length/2)),\n",
    "    nathb_cnt_arr_ls[1].reshape(-1, ntraj, int(length/2))), axis=2)\n",
    "nathb_cnt_arr = nathb_cnt_arr.reshape(-1, ntraj*length).T\n",
    "\n",
    "for i0, key in enumerate(type_hb_label):\n",
    "    cvs_df[key] = nathb_cnt_arr[:, i0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cvs_df.to_pickle(\"/project/dinner/kjeong/insulin/pipeline/step7_5ns/step1_cvs_state/cvs_df.pkl\")\n",
    "print(cvs_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CA_label = np.array([9, 12, 13, 16, 21, 23, 24, 25, 26, 29])\n",
    "mychoice_feat_resid = np.array(\n",
    "    [\n",
    "        [ 9,  9], [ 9, 13], [13,  9], [13, 13],#Alpha-Alpha\n",
    "        [24, 24], [24, 26], [26, 24], [26, 26],#Beta-Beta\n",
    "        [21, 29], [24, 29], [26, 21],#Turn-Cterm\n",
    "        [29, 21], [29, 24], [21, 26],#Cterm-Turn\n",
    "        [21,  9], [26,  9], [29,  9], [21, 16], [26, 16], [29, 16],\n",
    "        [ 9, 21], [ 9, 26], [ 9, 29], [16, 21], [16, 26], [16, 29]\n",
    "    ]\n",
    ")\n",
    "def res2id(elem):\n",
    "    return CA_label.tolist().index(elem)\n",
    "mychoice_feat = np.array(list(map(res2id, np.ravel(mychoice_feat_resid)))).reshape(mychoice_feat_resid.shape)\n",
    "\n",
    "def tanh_std(arr):\n",
    "    mu = np.mean(arr, axis=0)\n",
    "    sigma = np.std(arr, axis=0)\n",
    "    return np.tanh((arr-mu)/(2*sigma))\n",
    "\n",
    "def angle_std(arr):\n",
    "    mu_angle= np.mean(arr[dim_arr], axis=0)\n",
    "    return np.hstack((np.cos(np.deg2rad(arr-mu_angle)), np.sin(np.deg2rad(arr-mu_angle))))\n",
    "\n",
    "# Processed distance\n",
    "dist_0_arr=feat_0_arr[:, mychoice_feat[:, 0], mychoice_feat[:, 1]]\n",
    "dist_1_arr=feat_1_arr[:, mychoice_feat[:, 0], mychoice_feat[:, 1]]\n",
    "dist_arr = np.concatenate((dist_0_arr.reshape(ntraj, 500, 26), dist_1_arr.reshape(ntraj, 500, 26)), axis=1).reshape((2*Nframe, 26))\n",
    "proc_dist = tanh_std(dist_arr)\n",
    "\n",
    "# Processed angle\n",
    "angle_of_interest =[\"phi-beta(me)\", \"phi-alpha(me)\", \n",
    "                    \"phi-open(ch1)\", \"phi-open(ch2)\",\n",
    "                    \"theta\", \"phi\", \"Theta\", \"Phi\", \"Psi\"]\n",
    "proc_angle = angle_std(cvs_df.loc[:, angle_of_interest])\n",
    "proc_rcom = cvs_df.loc[:, 'r_com'].to_numpy().reshape(2*Nframe, 1)/2\n",
    "\n",
    "proc_arr = np.hstack((proc_rcom, proc_dist, proc_angle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_lab = np.array(\n",
    "    [\"r_com\"]+[f\"B{i:02d}-B'{j:02d}\" for i, j in mychoice_feat_resid] \\\n",
    "    +[f\"cos({angle})\" for angle in angle_of_interest]\\\n",
    "    +[f\"sin({angle})\" for angle in angle_of_interest]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_df = pd.DataFrame({lab: arr for lab, arr in zip(proc_lab, proc_arr.T)})\n",
    "#proc_df.to_pickle(f\"{workdir}/step7_5ns/step1_cvs_state/proc_df.pkl\")\n",
    "print(proc_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans with 1000 clusters is done (14.92 sec)\n",
      "Kmeans with 1500 clusters is done (18.80 sec)\n",
      "Kmeans with 2000 clusters is done (22.39 sec)\n",
      "Kmeans with 3000 clusters is done (34.80 sec)\n"
     ]
    }
   ],
   "source": [
    "# Slicing\n",
    "sliced_proc_arr = proc_arr.reshape((ntraj, length, 45))[:, 50:100, :].reshape((ntraj*50, 45))\n",
    "sliced_other_arr = np.ravel(other_arr.reshape((ntraj, length))[:, 50:100])\n",
    "\n",
    "# Number of Markov states `k_ls` and random state `rn_ls`\n",
    "#k_ls = [100, 200, 400, 600]\n",
    "k_ls = [1000, 1500, 2000, 3000]\n",
    "rn_ls = [0, 1, 2]\n",
    "neighbors_arr = np.zeros((len(k_ls), len(rn_ls)), dtype=object)\n",
    "for i0, k in enumerate(k_ls):\n",
    "    t0 = time()\n",
    "    for i1, rn in enumerate(rn_ls):\n",
    "        km = MiniBatchKMeans(n_clusters = k-2, random_state=rn).fit(sliced_proc_arr[sliced_other_arr])\n",
    "        neighbors_arr[i0, i1] = NearestNeighbors(n_neighbors=1).fit(km.cluster_centers_)\n",
    "    print(f\"Kmeans with {k} clusters is done ({time()-t0:.2f} sec)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 clusters is done (408.54 sec)\n",
      "1500 clusters is done (615.60 sec)\n",
      "2000 clusters is done (812.85 sec)\n",
      "3000 clusters is done (1225.99 sec)\n"
     ]
    }
   ],
   "source": [
    "# state_arr\n",
    "state_arr = np.zeros((len(k_ls), len(rn_ls), 2*Nframe), dtype=int)\n",
    "for i0, k in enumerate(k_ls):\n",
    "    t0 = time()\n",
    "    for i1, rn in enumerate(rn_ls):\n",
    "        state_other = np.squeeze(neighbors_arr[i0, i1].kneighbors(proc_arr[other_arr], return_distance=False))\n",
    "        state_arr[i0, i1, mon_arr] = k-1\n",
    "        state_arr[i0, i1, other_arr] = state_other+1\n",
    "    print(f\"{k} clusters is done ({time()-t0:.2f} sec)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(\"/project/dinner/kjeong/insulin/pipeline/step7_5ns/step1_cvs_state/state_arr.npy\", state_arr)\n",
    "#np.save(\"/project/dinner/kjeong/insulin/pipeline/step7_5ns/step1_cvs_state/state_arr_hk.npy\", state_arr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
